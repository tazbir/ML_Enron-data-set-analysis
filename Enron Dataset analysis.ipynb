{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing packages and setting roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir=\"D://Machine_learning_data_set/enron_mail_20150507.tar/maildir/lay-k/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running a loop through directory and printing informations regarding that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory, subdirectory, filenames in  os.walk(rootdir):\n",
    "    print(directory, subdirectory, len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading a file from directory to read and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.parser import Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read=\"D://Machine_learning_data_set/enron_mail_20150507.tar/maildir/lay-k/all_documents/1_\"\n",
    "with open(file_to_read, \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = Parser().parsestr(data)\n",
    "\n",
    "print(\"To: \" , email['to'])\n",
    "print(\" From: \" , email['from'])\n",
    " \n",
    "print(\" Subject: \" , email['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n \\n Body: \" , email.get_payload())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have learned how to read data from email of enron data set. the code below will create to_email_list with tabs, commas and spaces. this code can be used to see raw data fetched from the file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rootdir = \"D://Machine_learning_data_set/enron_mail_20150507.tar/maildir/lay-k/family/\"\n",
    "\n",
    "#this function takes 4 params the input email file, \n",
    "#a list of all To emails, a list for From emails, \n",
    "#and a list that contains the body(text) of the email.\n",
    "\n",
    "def email_analyse(inputfile, to_email_list, from_email_list, email_body):\n",
    "    with open(inputfile, \"r\") as f:\n",
    "        data = f.read()\n",
    " \n",
    "    email = Parser().parsestr(data)\n",
    "    \n",
    " #We append the 3 fields to their respective lists. So that the email[‘to’] is appended to the to_email_list.\n",
    "\n",
    "    to_email_list.append(email['to'])\n",
    "    from_email_list.append(email['from'])\n",
    " \n",
    "    email_body.append(email.get_payload())\n",
    "#end of function    \n",
    "to_email_list = []\n",
    "from_email_list = []\n",
    "email_body = []\n",
    "\n",
    "for directory, subdirectory, filenames in  os.walk(rootdir):\n",
    "    for filename in filenames:\n",
    "        email_analyse(os.path.join(directory, filename), to_email_list, from_email_list, email_body )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "looping through all files in sent directory and making lists out of the to, from and body related data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rootdir = \"D://Machine_learning_data_set/enron_mail_20150507.tar/maildir/lay-k/sent/\"\n",
    "\n",
    "#this function takes 4 params the input email file, \n",
    "#a list of all To emails, a list for From emails, \n",
    "#and a list that contains the body(text) of the email.\n",
    "\n",
    "def email_analyse(inputfile, to_email_list, from_email_list, email_body):\n",
    "    with open(inputfile, \"r\") as f:\n",
    "        data = f.read()\n",
    " \n",
    "    email = Parser().parsestr(data)\n",
    "    \n",
    " #We append the 3 fields to their respective lists. So that the email[‘to’] is appended to the to_email_list.\n",
    "    if email['to']:\n",
    "        email_to = email['to']\n",
    "        email_to = email_to.replace(\"\\n\", \"\")\n",
    "        email_to = email_to.replace(\"\\t\", \"\")\n",
    "        email_to = email_to.replace(\" \", \"\")\n",
    "        email_to = email_to.split(\",\")\n",
    "        \n",
    "    for email_to_1 in email_to:\n",
    "            to_email_list.append(email_to_1)\n",
    "    \n",
    "    from_email_list.append(email['from'])\n",
    " \n",
    "    email_body.append(email.get_payload())\n",
    "    \n",
    "to_email_list = []\n",
    "from_email_list = []\n",
    "email_body = []\n",
    "\n",
    "for directory, subdirectory, filenames in  os.walk(rootdir):\n",
    "    for filename in filenames:\n",
    "        email_analyse(os.path.join(directory, filename), to_email_list, from_email_list, email_body )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing to file the data sets we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"to_email_list.txt\", \"w\") as f:\n",
    "    for to_email in to_email_list:\n",
    "        if to_email:\n",
    "            f.write(to_email)\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "with open(\"from_email_list.txt\", \"w\") as f:\n",
    "    for from_email in from_email_list:\n",
    "        if from_email:\n",
    "            f.write(from_email)\n",
    "            f.write(\"\\n\")        \n",
    "\n",
    "with open(\"ken_lay_emails.txt\", \"w\") as f:\n",
    "    for email_bod in email_body:\n",
    "        if email_bod:\n",
    "            f.write(email_bod)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "print(\"\\nTo email adresses: \\n\")\n",
    "print(collections.Counter(to_email_list).most_common(10))\n",
    " \n",
    "print(\"\\nFrom email adresses: \\n\")\n",
    "print(collections.Counter(from_email_list).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "# Sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ken_lay_emails.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize_list = word_tokenize(data)\n",
    " \n",
    "useful_words = [word  for word in word_tokenize_list if word not in stopwords.words('English')]\n",
    "\n",
    "frequency = nltk.FreqDist(useful_words)\n",
    " \n",
    "print(frequency.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "word_punct_tokenizer_list = word_punct_tokenizer.tokenize(data)\n",
    "\n",
    "print(word_punct_tokenizer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "\n",
    "stemmer_porter = PorterStemmer()\n",
    "stemmer_lancaster = LancasterStemmer()\n",
    "stemmer_snowball = SnowballStemmer('english')\n",
    "\n",
    "formatted_row = '{:>16}' * (len(stemmers) + 1)\n",
    "print('\\n', formatted_row.format('WORD', *stemmers), '\\n')\n",
    "\n",
    "for word in word_punct_tokenizer_list:   \n",
    "    # print(word + \" = \" + stemmer_porter.stem(word))\n",
    "    stemmed_words = [stemmer_porter.stem(word), stemmer_lancaster.stem(word), stemmer_snowball.stem(word)]    \n",
    "\n",
    "    print(formatted_row.format(word, *stemmed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizers = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "\n",
    "lemmatizer_wordnet = WordNetLemmatizer()\n",
    "\n",
    "formatted_row = '{:>24}' * (len(lemmatizers) + 1)\n",
    "print('\\n', formatted_row.format('WORD', *lemmatizers), '\\n')\n",
    "\n",
    "for word in word_punct_tokenizer_list:    \n",
    "    lemmatized_words = [lemmatizer_wordnet.lemmatize(word, pos='n'),lemmatizer_wordnet.lemmatize(word, pos='v')]\n",
    "    print(formatted_row.format(word, *lemmatized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a text into chunks \n",
    "def splitter(data, num_words):    \n",
    "    words = data.split(' ')    \n",
    "    output = []\n",
    "    \n",
    "    cur_count = 0    \n",
    "    cur_words = []\n",
    "    \n",
    "    for word in words:        \n",
    "        cur_words.append(word)        \n",
    "        cur_count += 1\n",
    "        \n",
    "        if cur_count == num_words:            \n",
    "            output.append(' '.join(cur_words))            \n",
    "            cur_words = []            \n",
    "            cur_count = 0\n",
    "    \n",
    "    output.append(' '.join(cur_words) )    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__': \n",
    "    \n",
    "    # Read the data from the Brown corpus\n",
    "    # data = ' '.join(brown.words()[:10000])\n",
    "    data = ' '.join(word_punct_tokenizer_list[:10000])\n",
    "    \n",
    "    # Number of words in each chunk     \n",
    "    num_words = 1000\n",
    "    \n",
    "    chunks = []    \n",
    "    counter = 0\n",
    "    \n",
    "    text_chunks = splitter(data, num_words)    \n",
    "    print(\"refined sentences as a chunk for analysis =\", text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "if __name__=='__main__':    \n",
    "\n",
    "    data = ' '.join(word_punct_tokenizer_list[:10000])\n",
    "    \n",
    "    # Number of words in each chunk     \n",
    "    num_words = 1000  \n",
    "    \n",
    "    chunks = []    \n",
    "    counter = 0\n",
    "    \n",
    "    text_chunks = splitter(data, num_words)\n",
    "    \n",
    "    # Create a dictionary that is based on these text chunks\n",
    "    for text in text_chunks:        \n",
    "        chunk = {'index': counter, 'text': text}        \n",
    "        chunks.append(chunk)        \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "category_map = {\n",
    "\n",
    " 'misc.forsale':'Sales',\n",
    " 'talk.religion.misc':'religious'}\n",
    "\n",
    "training_data = fetch_20newsgroups(subset='train', categories=category_map.keys(), shuffle=True, random_state=7)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_termcounts = vectorizer.fit_transform(training_data.data)\n",
    "print(\"\\nDimensions of training data:\", X_train_termcounts.shape)\n",
    "\n",
    "# Training a classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "input_data = text_chunks\n",
    "\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_termcounts)\n",
    "\n",
    "# Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB().fit(X_train_tfidf, training_data.target)\n",
    "\n",
    "X_input_termcounts = vectorizer.transform(input_data)\n",
    "\n",
    "X_input_tfidf = tfidf_transformer.transform(X_input_termcounts)\n",
    "\n",
    "# Predict the output categories\n",
    "predicted_categories = classifier.predict(X_input_tfidf)\n",
    "\n",
    "# Print the outputs\n",
    "for sentence, category in zip(input_data, predicted_categories):    \n",
    "    print('\\nInput:', sentence, '\\nPredicted category:', category_map[training_data.target_names[category]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "def extract_features(word_list):    \n",
    "    return dict([(word, True) for word in word_list])\n",
    "\n",
    "if __name__=='__main__':    \n",
    "    # Load positive and negative reviews      \n",
    "    positive_fileids = movie_reviews.fileids('pos')    \n",
    "    negative_fileids = movie_reviews.fileids('neg')\n",
    "    \n",
    "features_positive = [(extract_features(movie_reviews.words(fileids=[f])), 'Positive') for f in positive_fileids]    \n",
    "features_negative = [(extract_features(movie_reviews.words(fileids=[f])), 'Negative') for f in negative_fileids]\n",
    "\n",
    "# Split the data into train and test (80/20)    \n",
    "threshold_factor = 0.8    \n",
    "threshold_positive = int(threshold_factor * len(features_positive))    \n",
    "threshold_negative = int(threshold_factor * len(features_negative))\n",
    "\n",
    "features_train = features_positive[:threshold_positive] + features_negative[:threshold_negative]    \n",
    "features_test = features_positive[threshold_positive:] + features_negative[threshold_negative:]      \n",
    "print(\"\\nNumber of training datapoints:\", len(features_train))\n",
    "print(\"Number of test datapoints:\", len(features_test))\n",
    "# Train a Naive Bayes classifier    \n",
    "classifier = NaiveBayesClassifier.train(features_train)    \n",
    "print(\"\\nAccuracy of the classifier:\", nltk.classify.util.accuracy(classifier, features_test))\n",
    "\n",
    "print(\"\\nTop 10 most informative words:\")\n",
    "for item in classifier.most_informative_features()[:10]:        \n",
    "    print(item[0])\n",
    "# Sample input reviews    \n",
    "input_reviews = text_chunks\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for review in input_reviews:        \n",
    "    print(\"\\nReview:\", review)\n",
    "    probdist = classifier.prob_classify(extract_features(review.split()))        \n",
    "    pred_sentiment = probdist.max()\n",
    "    \n",
    "    print(\"Predicted sentiment:\", pred_sentiment)\n",
    "    print(\"Probability:\", round(probdist.prob(pred_sentiment), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
